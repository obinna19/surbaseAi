{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T03:28:55.120772Z","iopub.execute_input":"2025-04-15T03:28:55.121062Z","iopub.status.idle":"2025-04-15T03:28:57.072304Z","shell.execute_reply.started":"2025-04-15T03:28:55.121030Z","shell.execute_reply":"2025-04-15T03:28:57.071083Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**LLM Agentic AI model**\n\nDesigned to analyse AI content, verify the context in quantization, evaluate the model parameter and measure accuracy of the response.\n\nlets dive In...","metadata":{}},{"cell_type":"code","source":"# %% [markdown]\n# # LLM Analysis CodeLab\n# **Structured LLM Evaluation Framework using Roadmap Sections**\n\n# %%\n#!pip install openai pydantic python-dotenv -q\n\n# %%\nimport os\nimport openai\nfrom pydantic import BaseModel, Field, validator\nfrom typing import List, Dict\nimport pandas as pd\nfrom IPython.display import display, Markdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Pydantic Models for Roadmap Analysis**\n\n\n**Key Features Matching the Roadmap:**\n1. **Section-Specific Analysis**  \n   - Direct integration with roadmap sections\n   - Custom prompts for each LLM development phase\n   - Pre-configured validation for technical sections","metadata":{},"attachments":{}},{"cell_type":"code","source":"%% [markdown]\n# ## Pydantic Models for Roadmap Analysis\n\n# %%\nROADMAP_SECTIONS = [\n    \"LLM Architecture\", \"Building an Instruction Dataset\", \n    \"Pre-training Models\", \"Supervised Fine-Tuning\",\n    \"Preference Alignment\", \"Evaluation\", \n    \"Quantization\", \"New Trends\"\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Roadmap Analysis Request**\n\n**Structured Validation**  \n   ```python\n   @validator('section')\n   def validate_section(cls, v):\n       if v not in ROADMAP_SECTIONS:\n           raise ValueError(f\"Invalid section. Choose from {ROADMAP_SECTIONS}\")\n       return v\n   ```\n   - Ensures analysis aligns with defined roadmap categories","metadata":{},"attachments":{}},{"cell_type":"code","source":"class RoadmapAnalysisRequest(BaseModel):\n    section: str = Field(..., description=\"Selected roadmap section\")\n    prompt: str = Field(..., min_length=10, max_length=1000)\n    examples: List[str] = Field(default_factory=list)\n    api_token: str = Field(..., min_length=40, max_length=60)\n\n    @validator('section')\n    def validate_section(cls, v):\n        if v not in ROADMAP_SECTIONS:\n            raise ValueError(f\"Invalid section. Choose from {ROADMAP_SECTIONS}\")\n        return v\n\n    @validator('api_token')\n    def validate_token(cls, v):\n        if not v.startswith('sk-'):\n            raise ValueError('Invalid API token format')\n        return v","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Roadmap Analysis Response**","metadata":{}},{"cell_type":"code","source":"class RoadmapAnalysisResponse(BaseModel):\n    clean_text: str\n    clean_post_stream: List[str]\n    section_metrics: Dict[str, float]\n    comparison_results: Dict[str, str]\n\n# %% [markdown]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Raodmap-Guided OpenAI Integration**","metadata":{}},{"cell_type":"code","source":"# ## Roadmap-Guided OpenAI Integration\n\n# %%\nSECTION_PROMPTS = {\n    \"LLM Architecture\": \"Analyze transformer architecture components and their relationships\",\n    \"Evaluation\": \"Evaluate model performance using perplexity and BLEU scores\",\n    \"Quantization\": \"Explain quantization techniques and their tradeoffs\"\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Analysis and parsing section / response of roadmap**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def analyze_roadmap_section(request: RoadmapAnalysisRequest) -> RoadmapAnalysisResponse:\n    openai.api_key = request.api_token\n    \n    system_msg = f\"\"\"You are an LLM scientist analyzing {request.section}. Structure response with:\n    1. clean_text: Summary using concepts from {SECTION_PROMPTS[request.section]}\n    2. clean_post_stream: Analysis steps with {request.examples} comparisons\n    3. metrics: Accuracy, Reliability, Effectiveness scores (0-1)\n    \"\"\"\n    \n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_msg},\n            {\"role\": \"user\", \"content\": request.prompt}\n        ],\n        temperature=0.7,\n        max_tokens=500\n    )\n    \n    return parse_roadmap_response(response.choices[0].message['content'], request)\n\ndef parse_roadmap_response(text: str, request: RoadmapAnalysisRequest) -> RoadmapAnalysisResponse:\n    parsed = {\n        \"clean_text\": \"\",\n        \"clean_post_stream\": [],\n        \"section_metrics\": {},\n        \"comparison_results\": {}\n    }\n    \n    current_section = None\n    for line in text.split('\\n'):\n        if \"clean_text:\" in line:\n            parsed[\"clean_text\"] = line.split(\"clean_text:\")[1].strip()\n        elif \"clean_post_stream:\" in line:\n            current_section = \"post_stream\"\n        elif \"metrics:\" in line:\n            current_section = \"metrics\"\n        elif \"comparison:\" in line:\n            current_section = \"comparison\"\n        else:\n            if current_section == \"post_stream\":\n                parsed[\"clean_post_stream\"].append(line.strip())\n            elif current_section == \"metrics\":\n                if \":\" in line:\n                    key, value = line.split(\":\")\n                    parsed[\"section_metrics\"][key.strip()] = float(value.strip())\n    \n    parsed[\"comparison_results\"] = compare_with_examples(parsed[\"clean_text\"], request.examples)\n    return RoadmapAnalysisResponse(**parsed)\n\n# %% [markdown]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Roadmap-Specific Analysis Metrics**\n\n**Roadmap-Aligned Metrics**  \n   - Quantization-specific evaluation\n   - Architecture component tracking\n   - Training methodology comparisons","metadata":{},"attachments":{}},{"cell_type":"code","source":"# ## Roadmap-Specific Analysis Metrics\n\n# %%\ndef compare_with_examples(output: str, examples: List[str]) -> Dict[str, str]:\n    comparisons = {}\n    for idx, ex in enumerate(examples):\n        comparisons[f\"example_{idx+1}\"] = f\"Match: {len(set(output.split()) & set(ex.split()))} common terms\"\n    return comparisons\n\nclass RoadmapEvaluator:\n    def __init__(self, response: RoadmapAnalysisResponse):\n        self.response = response\n        \n    def calculate_accuracy(self) -> float:\n        return self.response.section_metrics.get('accuracy', 0)\n    \n    def calculate_reliability(self) -> float:\n        return len(self.response.clean_post_stream)/10\n    \n    def get_metrics(self) -> Dict[str, str]:\n        return {\n            \"Accuracy\": f\"{self.calculate_accuracy()*100:.1f}%\",\n            \"Reliability\": f\"{self.calculate_reliability()*10:.1f}/10\",\n            \"Effectiveness\": f\"{len(self.response.clean_text.split())/100:.1f}\"\n        }\n\n# %% [markdown]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Example usage with Roadmap sections**\n\nModify `analysis_request` parameters to test different sections\n\n**Usage Example:**\n```python\nanalysis_request = RoadmapAnalysisRequest(\n    section=\"Evaluation\",\n    prompt=\"Analyze perplexity metrics from the roadmap resources\",\n    examples=[\n        \"Perplexity calculation methods\",\n        \"BLEU score limitations\"\n    ],\n    api_token=API_TOKEN\n)\n```","metadata":{},"attachments":{}},{"cell_type":"code","source":"# ## Example Usage with Roadmap Sections\n\n# %%\n# Set in Kaggle Secrets\nAPI_TOKEN = \"your-api-key-here\"\n\n# %%\nanalysis_request = RoadmapAnalysisRequest(\n    section=\"Quantization\",\n    prompt=\"Explain 4-bit quantization techniques from the roadmap\",\n    examples=[\n        \"GPTQ quantization method\",\n        \"llama.cpp quantization approaches\"\n    ],\n    api_token=API_TOKEN\n)\n\n# %%\ntry:\n    response = analyze_roadmap_section(analysis_request)\n    evaluator = RoadmapEvaluator(response)\n    \n    display(Markdown(f\"### {analysis_request.section} Analysis\"))\n    display(Markdown(f\"**Clean Text:**\\n{response.clean_text}\"))\n    \n    display(Markdown(\"### Analysis Process\"))\n    display(pd.DataFrame({\n        \"Step\": response.clean_post_stream,\n        \"Stage\": [\"Processing\"]*len(response.clean_post_stream)\n    }))\n    \n    display(Markdown(\"### Quality Metrics\"))\n    display(pd.DataFrame({\n        \"Metric\": evaluator.get_metrics().keys(),\n        \"Value\": evaluator.get_metrics().values()\n    }))\n    \n    display(Markdown(\"### Example Comparisons\"))\n    display(pd.DataFrame(response.comparison_results.items()))\n    \nexcept Exception as e:\n    display(Markdown(f\"**Error:** {str(e)}\"))\n\n# %% [markdown]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Testing Framework**","metadata":{}},{"cell_type":"code","source":"# ## Testing Framework\n\n# %%\ndef test_roadmap_validation():\n    try:\n        RoadmapAnalysisRequest(\n            section=\"Invalid Section\",\n            prompt=\"Test\",\n            examples=[],\n            api_token=\"invalid\"\n        )\n    except ValueError as e:\n        assert \"section\" in str(e)\n\ndef test_quantization_analysis():\n    test_request = RoadmapAnalysisRequest(\n        section=\"Quantization\",\n        prompt=\"Test\",\n        examples=[],\n        api_token=\"sk-testtoken\"\n    )\n    assert test_request.section == \"Quantization\"\n\n# %%\ntest_roadmap_validation()\ntest_quantization_analysis()\nprint(\"Roadmap validation tests passed!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**This implementation directly connects to the LLM Scientist Roadmap through:\n- Section-specific validation and analysis\n- Resource-informed response generation\n- Technical metric calculations aligned with roadmap concepts\n- Structured comparison of implementation approaches\n- Version-aware analysis for \"New Trends\" section updates**","metadata":{}}]}